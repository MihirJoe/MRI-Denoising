{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0779cb53-a916-41f9-ab95-c24039d89a16",
   "metadata": {},
   "source": [
    "___\n",
    "# **BME 5710 project report**\n",
    "## Instructor -- Rizwan Ahmad (ahmad.46@osu.edu)\n",
    "## BME5710 -- Spring 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca22128-d705-4915-9165-1138474e2eaf",
   "metadata": {},
   "source": [
    "___\n",
    "### Provide descriptive answers at `???` locations and insert figures or tables at `?content?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d1a16-3368-4f9f-94d1-771dba37e494",
   "metadata": {},
   "source": [
    "___\n",
    "### Write your name below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f14effb-5721-472d-b299-c311b1edb084",
   "metadata": {},
   "source": [
    "Answer: Mihir Joshi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a46acc",
   "metadata": {},
   "source": [
    "___\n",
    "### Write the name of your teammates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b756447a",
   "metadata": {},
   "source": [
    "Answer: Dema & Jenna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af677ae3-4e2b-49df-a06a-771af1154d91",
   "metadata": {},
   "source": [
    "___\n",
    "### (1.1) Provide a layout of your CNN (6%)\n",
    "\n",
    "#### The layout should provide all the necessary details about the CNN architecture including number of channels, size of convolution kernels, activation functions, etc. For inspiration, see examples [here](https://www.geeksforgeeks.org/u-net-architecture-explained/), [here](https://www.researchgate.net/figure/The-architecture-of-Unet_fig2_334287825), [here](https://www.researchgate.net/figure/Modified-U-net-network-architecture_fig2_356216368), and [here](https://open-instruction.com/dl-algorithms/overview-of-residual-neural-network-resnet/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738302d6-ccf3-49af-85c7-eac2de4ed5e2",
   "metadata": {},
   "source": [
    "![alt text](<BME 5710 CNN Diagram.jpeg>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53876c54",
   "metadata": {},
   "source": [
    "___\n",
    "### (1.2) List *all* non-trivial features of your CNN and the training process. This may include use of dropout, learning rate scheduling, transfer learning, data augmentation, etc. Don't include items that are already covered in the layout provided above.  (1%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f65401",
   "metadata": {},
   "source": [
    "Answer: \n",
    "1) Dropout (p=0.1)\n",
    "2) Up sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bc63e9",
   "metadata": {},
   "source": [
    "___\n",
    "### (2) Answer the following questions about your CNN architecture and training. (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819917d8-0b7d-4cf9-9636-60764389eb90",
   "metadata": {},
   "source": [
    "___\n",
    "### (2.1) Provide at least ten hyperparameters that you *could* optimize in your CNN design and training. (1%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82087b67-e000-489a-95ff-0015550911a5",
   "metadata": {},
   "source": [
    "Answer: \n",
    "1) Learning rate, \n",
    "2) number of epochs, \n",
    "3) number of convolutional layers, \n",
    "4) batch size, \n",
    "5) kernel size, \n",
    "6) stride, \n",
    "7) padding, \n",
    "8) dropout, \n",
    "9) number of input and output channels,\n",
    "10) pooling layer type, \n",
    "11) pooling layer size, \n",
    "12) type of optimizer, \n",
    "13) regularizing term, \n",
    "14) type of activiation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604e191e-148d-4da1-89e7-f44e40cff7f0",
   "metadata": {},
   "source": [
    "___\n",
    "### (2.2) Now, list the hyperparameters that you *have* optimized. (1%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f0d19-fca5-45cc-a9d9-d9cb87dde1cf",
   "metadata": {},
   "source": [
    "Answer: \n",
    "1) number of epochs, \n",
    "2) learning rate, \n",
    "3) dropout, \n",
    "4) number of convolutional layers, \n",
    "5) activation function (ReLU), \n",
    "6) batch size, \n",
    "7) kernel size, \n",
    "8) stride, \n",
    "9) pooling size, \n",
    "10) number of input and output channels "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbfa6b3-c2e7-48af-bf3a-88d912c015ab",
   "metadata": {},
   "source": [
    "___\n",
    "### (2.3) Describe your stretegy/approach for optimizing hyperparameters. (1%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5850bf-b2b9-4541-8c10-cc5cd9a4651a",
   "metadata": {},
   "source": [
    "Answer: Before optimizing the hyperparameters, we first set up the NMSE and SSIM functions to get a sense of what the errors would look like. To test that these functions would work, a small number of epochs (~3 epochs) were used to see fluctuation on the plots. A simple model, with a few convolution layers (no pooling) was created along with using the given epochs, batch_size, and learning rate. This initial model was then used as a baseline. Next, the depth of the network was modified, along with the number of input and output channels. Once the loss functions converged to a certain value, pooling and upsampling (transpose) was also added to mimic an autoencoder architecture. The kerenel size was adjusted, but it was found that 3 provided the best results. The number of convolutional layers was adjusted based on the loss curve for these changes. This systematic approach enabled the team to understand which hyperparameter had the largest effect on the network. Finally, to further improve model performance a skip connection along with dropout was implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9d7bc2-ed30-432f-bf37-556ca4bdd575",
   "metadata": {},
   "source": [
    "___\n",
    "### (2.4) What loss function and optimizers did you use?  Express the loss function mathematically. (1%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c29d8db-783d-45f6-aef1-d63b16c29570",
   "metadata": {},
   "source": [
    "Answer: \n",
    "1) ${\\sf{MSE}}(\\boldsymbol{y}, \\widehat{\\boldsymbol{y}}) =  \\frac{\\|\\boldsymbol{y} - \\widehat{\\boldsymbol{y}}\\|_2^2}{n}$\n",
    "2) Adam Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f12df09-b84d-47d7-a0a8-52356975c8d3",
   "metadata": {},
   "source": [
    "___\n",
    "### (2.5) Calculate the number of learnable parameters in your final CNN. How does that number compare with the number of training samples? Is your network overfitting or underfitting and how did you arrive at that conclusion? Explain that in the context of loss vs. epoch and NMSE vs. epoch curves for training and validation datasets. (1%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90446deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3931920"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of learnable parameters\n",
    "(3*3*1*16) + (3*3*1*64) + (3*3*16*32) + (3*3*32*64) + (3*3*64*128) + (3*3*128*256) + (3*3*256*256) + (3*3*256*512) + (3*3*512*256) + (3*3*256*128) + (3*3*192*128) + (3*3*128*64) + (3*3*64*1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318136f5-a027-4128-a481-9e90d7ea85b6",
   "metadata": {},
   "source": [
    "Answer: The number of training samples is 52 images. The calcuated learnable parameters is much larger than size of the training set. The network seems to be overfitting (especially after 90 epochs) because the training loss continues to decrease while the validation NMSE and loss remain relatively the same as the number of epochs increase (see the plot below). The MSE loss for the training set decreases (following the same trend as the NMSE curve) with higher epochs. For the validation set, the convergence happens earlier. In our notebook, it is submitted for 100 epochs, but the following plot shows the model running for 200 epochs to determine if there is over/underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ffb777",
   "metadata": {},
   "source": [
    "![alt text](UniqueNetLoss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddb81cd",
   "metadata": {},
   "source": [
    "___\n",
    "### (3.1) Insert (or draw using Markdown) a table that summarizes NMSE and SSIM for (i) noisy images, (ii) images denoised with TrivialNet model included in `starter_code.ipynb`, and (iii) images denoised with your CNN. Include the metrics from training, validation, and test datasets. (6%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62727b26",
   "metadata": {},
   "source": [
    "|              | Noisy Image | TrivialNet | UniqueNet |\n",
    "|--------------|-------------|------------|--------|\n",
    "| **Training**     |   NMSE: -7.892          |     NMSE: -15.634       |  NMSE: -19.108      |\n",
    "| **Training**     |       SSIM: 0.321      |       SSIM: 0.643     |    SSIM:    0.862|\n",
    "| **Validation**     |    NMSE: -7.946         |  NMSE: -15.751          |   NMSE:   -19.055  |\n",
    "| **Validation**     |      SSIM: 0.335       |       SSIM: 0.654     |      SSIM:  0.855 |\n",
    "| **Testing**     | NMSE: -7.982            |    NMSE: -15.819        |      NMSE: -19.273 |\n",
    "| **Testing**     |         SSIM: 0.321    |     SSIM: 0.640       |    SSIM:    0.86|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c308882b",
   "metadata": {},
   "source": [
    "___\n",
    "### (3.2) When it comes to comparing images, what does SSIM capture that NMSE does not? (1%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814291f",
   "metadata": {},
   "source": [
    "Answer: Structural Similarity Index Measure (SSIM) is a quantitative way of capturing the perceived quality of an image. SSIM primarily captures spatial dependencies and structural features of two images together and outputs a score. So, if SSIM is calculated for the same image, the value will be 1. NMSE is more of an error calculation. Here, NMSE can be thought of as the signial-to-noise ratio which is a representation of how much noise is present in the image versus the desired signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f83ccbb",
   "metadata": {},
   "source": [
    "___\n",
    "### (4.1) Display a figure where the first row (from left to right) shows an example of clean image, noisy image, image denoised with TrivialNet, and the image denoised with your CNN, and the second row shows corresponding error maps after 3-fold amplification. Select the image from the test dataset. (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a31af64",
   "metadata": {},
   "source": [
    "#### TrivialNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc338025",
   "metadata": {},
   "source": [
    "![alt text](TrivialNet_100_images.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5ad79b",
   "metadata": {},
   "source": [
    "#### UniqueNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb7daed",
   "metadata": {},
   "source": [
    "![alt text](UniqueNetImages.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c89ab4-45de-4bd2-ac00-57eed8ed6e95",
   "metadata": {},
   "source": [
    "___\n",
    "### (4.2) From (4.1), identify which image features are well-preserved by your denoiser and which are lost. Additionally, describe how you could further improve the performance of your denoiser if given more time and resources. (1%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c3b816",
   "metadata": {},
   "source": [
    "Answer: The image features that are well preserved by the denoiser were the holes in the bones and the general shape of the brain. However, the network was unable to preserve the finer edge details of the gray and white matter. To account for this, specific filters could be used to extract these fine edge features and include them in the model training. The denoiser could be improved by add more max pooling and upsampling layers to create a larger bottleneck. This will enhance the autoencoder architecture as the image will become compressed and the network will need to work harder to decompress it. However, to prevent overfitting, more dropout layers and batch normalization can be included to improve the denoising performance. It would be useful to also compare this network to the literature and take inspiration from models like U-Net to understand the intentional choices behind the layers. Furthermore, diffusion models could be applied to systematically add noise at each training step and remove it to force the network to learn the underlying noise impact on the image. These improvements would make the model more generalizable with new MRI data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f8cc1",
   "metadata": {},
   "source": [
    "Thank you for a great semester! :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
